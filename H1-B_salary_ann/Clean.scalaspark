// spark - submit
def replace_title(title: String): String = {
    if (title startsWith "SOFTWARE") return "SOFTWARE ENGINEER"
    else if (title startsWith "WEB DEVELOPER") return"WEB DEVELOPER"
    else if (title startsWith "DATA ENGINEER") return "DATA ENGINEER"
    else if (title startsWith "DATA SCIENTIST") return "DATA SCIENTIST"
    else if (title startsWith "DATA ANALYST") return "DATA ANALYST"
    "NONE"
}

val raw = sc.textFile("/user/mt4050/h1b_salary.txt")

// drop the header
val raw_no_header = raw.mapPartitionsWithIndex {(idx, iter) => if (idx == 0) iter.drop(1) else iter }
// raw_no_header.take(10).foreach(println)

// extract the fields we want
// (employer, job_title, base_salary, visa_status, state, year, month)
val raw_split = raw_no_header.map(line => ((line.split(','))(1), (line.split(','))(2), (line.split(','))(3), (line.split(','))(7), (line.split(','))(8), (line.split(','))(9), (line.split(','))(10)))
// raw_split.take(10).foreach(println)

// identify job title
val job = raw_split.map(x => (x._1, replace_title(x._2), x._3, x._4, x._5, x._6, x._7))
// job.take(10).foreach(println)

// filter len != 7
val output = job.map{case(employer, job_title, base_salary, visa_status, state, year, month) => employer + "," + job_title+ "," + base_salary+ "," + visa_status+ "," + state+ "," + year+ "," + month}
val filter_output = output.filter(line => (line.split(',')).length == 7)
// filter_output.take(10).foreach(println)

// save to txt
filter_output.saveAsTextFile("/user/mt4050/h1b_salary_clean.txt")


// hdfs dfs -get h1b_salary_clean.txt
// scp mt4050@dumbo.es.its.nyu.edu:/home/mt4050/h1b_salary_clean.txt ./h1b_salary_clean.txt