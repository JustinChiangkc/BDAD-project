Describe your directories and files, how to build your code, how to run your code, where to find results of a run.

â€¢ Identify where we can find the input data that you used. If we cannot locate it, we may ask for access to input data, or
we may ask you to put it on Dumbo if it isn't already there.



## for salary-rate linear regression model & hypothesis testing
    
    a. data ingest : 
        run "python3 final_application_code/data_ingest/salary_data_ingest.py" in command line

    b. etl : 
        go to final_application_code/etl_code/salary_etl.scala, and execute the code in spark REPL

    c. profiling : 
        go to final_application_code/profiling_code/salary_profiling.scala, and execute the code in spark REPL

    d. analytics : 
        i. get the median of salary 
            -> go to final_application_code/app_code/salary_get_salary_median.scala, and execute the code in spark REPL
            
        ii. get the result of linear equation and hypothesis
            -> go to final_application_code/app_code/salary_linear_regression_model.scala, and execute the code in spark REPL

    e. result : 
        see final_application_code/screenshots/salary_linear_regression_result.png

## for pl/skills vs salary
    a. data ingest:
	run "python3 final_application_code/data_ingest/scraper/main.py" in command line
	requires selenium installed. requires to put linkedin credential in linkedinconfig.txt.
	It will automatically scraped from linkedin. Built based on early April's website layer
	
    b. etl
	go to final_application_code/eta_code/pl_etl.scala, and execute code in spark REPL
	It will generate the ((company_name, role), (pl_bitmap, degree_bitmap))

	go to final_application_code/etl_code/skills_etl.scala, and execute code in spark REPL
	It will generate the ((company_name, role), (skills_bitmap, degree_bitmap))

    c. Profiling
	first, go to final_application_code/etl_code/join.scala, and execute codes in spark REPL
	It will join the salary and pl/skills based on (company_name, role)

	then go to final_application_code/profiling_code/visualization_pl_skills, and execute all codes in spark REPL
	It will generate the data after sorting into different categories. They are ready to be visualized on Tableau

    d. Analytics
	go to final_application_code/eta_code/pl_importance_test.scala, and execute code in spark REPL
	It attempts to build a linear regression model for importance test but failed
	
	Most analytics with aggregate methods(ex.distinct, count, median, average) are done on tableau

    e, Result
	You can find several screenshots of the final result under final_application_code/screenshots/visualization_pl_skills



